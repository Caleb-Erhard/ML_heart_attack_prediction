{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f3700fb-e874-4718-bbdc-963b7ae9e744",
   "metadata": {},
   "source": [
    "### Imports & Core Setup  \n",
    "Load NumPy/Pandas, scikit-learn metrics and pipeline components, and kNN.  \n",
    "This cell defines the core toolbox the rest of the notebook relies on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87193fda-a962-4204-94b4-f270dd9dd79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, log_loss, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss, accuracy_score, confusion_matrix, classification_report\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import log_loss, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c841f9-3460-4421-a3e6-0ce3a54c7fa8",
   "metadata": {},
   "source": [
    "### CSV I/O Helpers  \n",
    "Convenience code for loading separate train/test CSVs and performing light cleaning  \n",
    "if the instructor provided external files. Safe to skip if `X_train` / `y_train` are already defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c017e9c7-b45b-41c7-8bd1-1e9875b34d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (14470, 19)\n",
      "Test shape: (230, 19)\n"
     ]
    }
   ],
   "source": [
    "# separate train/test CSVs:\n",
    "df_train = pd.read_csv(\"NHANES_Data_P1_MI_train.csv\")\n",
    "df_test  = pd.read_csv(\"NHANES_Data_P1_MI_test.csv\")\n",
    "\n",
    "print(\"Train shape:\", df_train.shape)\n",
    "print(\"Test shape:\", df_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcf0520-d8e9-482a-9446-1ef4417bdc59",
   "metadata": {},
   "source": [
    "### Leakage Guard: Drop Non-Features  \n",
    "Define and drop columns the model must not see (IDs, labels, or any obvious target leakage).  \n",
    "This keeps the design matrix clean and reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33b2a0f1-1fce-40d2-8969-86dc9aeff129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X columns: ['Sex', 'Age', 'Race', 'Systolic', 'Diastolic', 'Pulse', 'BMI', 'HDL', 'Trig', 'TCHOL', 'eGFR', 'CurrentSmoker', 'Diabetes']\n",
      "Class balance (train): {0: 0.9596891342242883, 1: 0.040310865775711795}\n",
      "Shapes: (13768, 13) (13768,)\n"
     ]
    }
   ],
   "source": [
    "# Columns we do NOT want the model to see\n",
    "DROP_COLS = [\"ID\", \"Insurance\", \"Edu\", \"Income\", \"LDL\"]\n",
    "\n",
    "# Map MI to {0,1}: 1 = MI (positive), 2 = No MI (negative)\n",
    "mi_mapped = df_train[\"MI\"].map({1: 1, 2: 0})\n",
    "mask = mi_mapped.notna()\n",
    "\n",
    "y = mi_mapped[mask].astype(int).reset_index(drop=True)\n",
    "X = df_train.loc[mask].drop(columns=DROP_COLS + [\"MI\"], errors=\"ignore\").reset_index(drop=True)\n",
    "\n",
    "print(\"X columns:\", list(X.columns))\n",
    "print(\"Class balance (train):\", y.value_counts(normalize=True).to_dict())\n",
    "print(\"Shapes:\", X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae1c6ee-f335-4d83-939b-5bac972456f0",
   "metadata": {},
   "source": [
    "### Feature Engineering: Interactions  \n",
    "Create clinically-sensible interaction features (e.g., Age×BP, Age×Lipids, lipid ratios).  \n",
    "These transformations can sharpen distances for kNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24817baa-005a-4374-a60a-f62c5bc67d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_interactions(X_arr_or_df):\n",
    "    # Accept ndarray or DataFrame, return DataFrame\n",
    "    if isinstance(X_arr_or_df, pd.DataFrame):\n",
    "        Xdf = X_arr_or_df.copy()\n",
    "    else:\n",
    "        Xdf = pd.DataFrame(X_arr_or_df)\n",
    "\n",
    "    # Guard: only add if columns exist\n",
    "    if {\"Age\",\"Systolic\"}.issubset(Xdf.columns):\n",
    "        Xdf[\"age_x_Systolic\"] = Xdf[\"Age\"] * Xdf[\"Systolic\"]\n",
    "    if {\"Age\",\"TCHOL\"}.issubset(Xdf.columns):\n",
    "        Xdf[\"age_x_TCHOL\"] = Xdf[\"Age\"] * Xdf[\"TCHOL\"]\n",
    "    if {\"Age\",\"HDL\"}.issubset(Xdf.columns):\n",
    "        Xdf[\"age_x_HDL\"] = Xdf[\"Age\"] * Xdf[\"HDL\"]\n",
    "    if \"Age\" in Xdf.columns and \"CurrentSmoker\" in Xdf.columns:\n",
    "        # CurrentSmoker is coded 1/2 in the spec; if your preprocessing made it 0/1 already, this still works\n",
    "        smoker_binary = (Xdf[\"CurrentSmoker\"] == 1).astype(int)\n",
    "        Xdf[\"age_x_smoker\"] = Xdf[\"Age\"] * smoker_binary\n",
    "\n",
    "    return Xdf\n",
    "\n",
    "interaction_tf = FunctionTransformer(add_interactions, validate=False)\n",
    "\n",
    "# Categorical columns to one-hot\n",
    "CAT_COLS = [\"Sex\", \"Race\", \"CurrentSmoker\", \"Diabetes\"]\n",
    "\n",
    "# Robust OHE across sklearn versions\n",
    "def make_ohe():\n",
    "    try:\n",
    "        return OneHotEncoder(drop=None, handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(drop=None, handle_unknown=\"ignore\", sparse=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ae77a0-31fb-4e61-9f1b-4e6ef66e2b02",
   "metadata": {},
   "source": [
    "### Preprocessing Pipeline  \n",
    "Use a ColumnTransformer to:  \n",
    "- Impute missing values  \n",
    "- StandardScale numeric features  \n",
    "- One-Hot Encode categorical features  \n",
    "\n",
    "This preprocessing is applied consistently for training and inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e56cc708-6e8a-495e-9f39-f879af5fe3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline is ready.\n"
     ]
    }
   ],
   "source": [
    "ohe = make_ohe()\n",
    "\n",
    "numeric_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", ohe)\n",
    "])\n",
    "\n",
    "# After interactions, select by dtype so interaction columns are treated as numeric\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", categorical_pipe, CAT_COLS),\n",
    "        (\"num\", numeric_pipe, selector(dtype_include=np.number)),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"interactions\", interaction_tf),\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"clf\", KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"clf__n_neighbors\": [17],\n",
    "    \"clf__weights\": [\"uniform\"],\n",
    "    \"clf__p\": [2]  # Manhattan often wins on tabular; try fractional too\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "print(\"Pipeline is ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd6f3f7-00c8-41b6-9c83-6aac93d3ed5d",
   "metadata": {},
   "source": [
    "### Train/Validation Split  \n",
    "Stratified split into:  \n",
    "- **Imbalanced TRAIN** (reflects original dataset, ~96/4)  \n",
    "- **Balanced VALIDATION** (~50/50 distribution)  \n",
    "\n",
    "The balanced validation slice mimics the grader’s conditions for accuracy/log-loss scoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a15e3c73-0103-4e53-8b7d-4f1fd8036f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (11014, 13) (11014,)\n",
      "Valid (orig) balance: {0: 0.9596949891067538, 1: 0.04030501089324619}\n",
      "Valid (balanced) balance: {0: 0.5, 1: 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Stratified split preserves original imbalance in training portion\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "# Build a ~50/50 validation set to mimic the test distribution\n",
    "pos_valid = X_valid[y_valid == 1]\n",
    "neg_valid = X_valid[y_valid == 0]\n",
    "k = min(len(pos_valid), len(neg_valid))\n",
    "\n",
    "pos_valid = pos_valid.sample(n=k, random_state=RANDOM_STATE)\n",
    "neg_valid = neg_valid.sample(n=k, random_state=RANDOM_STATE)\n",
    "\n",
    "X_valid_bal = pd.concat([pos_valid, neg_valid])\n",
    "y_valid_bal = pd.concat([y_valid.loc[pos_valid.index], y_valid.loc[neg_valid.index]])\n",
    "\n",
    "# Shuffle\n",
    "perm = np.random.RandomState(RANDOM_STATE).permutation(len(X_valid_bal))\n",
    "X_valid_bal = X_valid_bal.iloc[perm]\n",
    "y_valid_bal = y_valid_bal.iloc[perm]\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Valid (orig) balance:\", y_valid.value_counts(normalize=True).to_dict())\n",
    "print(\"Valid (balanced) balance:\", y_valid_bal.value_counts(normalize=True).to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd23e99-35da-4746-aff4-5d9a703f9056",
   "metadata": {},
   "source": [
    "### Undersampling for kNN Fit  \n",
    "Construct a balanced training set (50/50) by randomly undersampling the majority class.  \n",
    "This prevents kNN neighbors from being dominated by negatives and preserves minority signal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "41f5e0b8-0282-41b8-a463-cd0bb434e191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train balance: {0: 0.959687670237879, 1: 0.04031232976212094}\n",
      "Undersampled train balance: {1: 0.5, 0: 0.5}\n",
      "Undersampled shapes: (888, 13) (888,)\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "=== Best params (undersampled) ===\n",
      "{'clf__n_neighbors': 17, 'clf__p': 2, 'clf__weights': 'uniform'}\n",
      "Best CV log loss: 0.6500531166794496\n"
     ]
    }
   ],
   "source": [
    "# 1:1 undersampling on the training split\n",
    "pos_idx = y_train[y_train == 1].index\n",
    "neg_idx = y_train[y_train == 0].index\n",
    "\n",
    "n_pos = len(pos_idx)\n",
    "neg_keep = np.random.RandomState(RANDOM_STATE).choice(neg_idx, size=n_pos, replace=False)\n",
    "keep_idx = np.concatenate([pos_idx, neg_keep])\n",
    "\n",
    "X_train_us = X_train.loc[keep_idx]\n",
    "y_train_us = y_train.loc[keep_idx]\n",
    "\n",
    "# Shuffle\n",
    "perm = np.random.RandomState(RANDOM_STATE).permutation(len(X_train_us))\n",
    "X_train_us = X_train_us.iloc[perm]\n",
    "y_train_us = y_train_us.iloc[perm]\n",
    "\n",
    "print(\"Original train balance:\", y_train.value_counts(normalize=True).to_dict())\n",
    "print(\"Undersampled train balance:\", y_train_us.value_counts(normalize=True).to_dict())\n",
    "print(\"Undersampled shapes:\", X_train_us.shape, y_train_us.shape)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"neg_log_loss\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid.fit(X_train_us, y_train_us)\n",
    "\n",
    "print(\"\\n=== Best params (undersampled) ===\")\n",
    "print(grid.best_params_)\n",
    "print(\"Best CV log loss:\", -grid.best_score_)\n",
    "\n",
    "best_model = grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7785f9c-1e86-4911-91fc-a211c33fc419",
   "metadata": {},
   "source": [
    "### Validation Scoring @ t=0.50  \n",
    "Evaluate the kNN model on the balanced validation split:  \n",
    "- Accuracy and log loss  \n",
    "- Confusion matrix  \n",
    "- Quick threshold table (even though the grading threshold is fixed at 0.50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "120cb934-0cac-4bb5-895c-2ee737b31fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy @ t=0.50: 0.7748\n",
      "Log loss:          0.4994\n",
      "Confusion matrix:\n",
      " [[82 29]\n",
      " [21 90]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.796     0.739     0.766       111\n",
      "           1      0.756     0.811     0.783       111\n",
      "\n",
      "    accuracy                          0.775       222\n",
      "   macro avg      0.776     0.775     0.774       222\n",
      "weighted avg      0.776     0.775     0.774       222\n",
      "\n",
      "\n",
      "Threshold table:\n",
      "  t  positives  pos_rate\n",
      "0.1        201  0.905405\n",
      "0.2        178  0.801802\n",
      "0.3        154  0.693694\n",
      "0.4        147  0.662162\n",
      "0.5        119  0.536036\n",
      "0.6         85  0.382883\n",
      "0.7         67  0.301802\n",
      "0.8         37  0.166667\n",
      "0.9          7  0.031532\n"
     ]
    }
   ],
   "source": [
    "probs = best_model.predict_proba(X_valid_bal)[:, 1]\n",
    "preds = (probs >= 0.50).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_valid_bal, preds)\n",
    "ll  = log_loss(y_valid_bal, probs)\n",
    "cm  = confusion_matrix(y_valid_bal, preds)\n",
    "\n",
    "print(f\"Accuracy @ t=0.50: {acc:.4f}\")\n",
    "print(f\"Log loss:          {ll:.4f}\")\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "print(classification_report(y_valid_bal, preds, digits=3))\n",
    "\n",
    "# Threshold sweep\n",
    "rows = []\n",
    "for t in np.linspace(0.1, 0.9, 9):\n",
    "    pp = (probs >= t).astype(int)\n",
    "    rows.append({\"t\": round(t, 2), \"positives\": int(pp.sum()), \"pos_rate\": float(pp.mean())})\n",
    "thr_table = pd.DataFrame(rows)\n",
    "print(\"\\nThreshold table:\")\n",
    "print(thr_table.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e46b228-4c90-40d5-bc58-9688659867f0",
   "metadata": {},
   "source": [
    "### Final Model Selector  \n",
    "Choose the exact kNN configuration for submission:  \n",
    "- k = 17  \n",
    "- p = 2 (Euclidean distance)  \n",
    "- weights = \"uniform\"  \n",
    "\n",
    "Downstream code assumes this object name for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fff4b570-64ba-4eb3-aa2c-e344dedef04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FINAL MODEL PICK ===\n",
    "final_model = best_model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bac761-2716-4cfb-b6e0-0c6e2e8d7645",
   "metadata": {},
   "source": [
    "### Sanity Probe on Validation  \n",
    "Re-check the chosen `final_model` on the balanced validation set to confirm  \n",
    "it still produces the expected ~0.75 accuracy and ~0.499 log loss.  \n",
    "If this drifts, you may be pointing at the wrong model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d41d4f9-1792-444e-bbcc-38a554ba620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_table(probs, t_values=(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9)):\n",
    "    rows = []\n",
    "    for t in t_values:\n",
    "        preds = (probs >= t).astype(int)\n",
    "        rows.append({\"t\": t, \"positives\": int(preds.sum()), \"pos_rate\": float(preds.mean())})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def summarize_predictions(name, y_true, probs, t=0.5):\n",
    "    print(f\"\\n=== {name} @ t={t:.2f} ===\")\n",
    "    preds = (probs >= t).astype(int)\n",
    "    if y_true is not None:\n",
    "        print(f\"Accuracy: {accuracy_score(y_true, preds):.6f}\")\n",
    "        print(f\"Log loss: {log_loss(y_true, probs):.6f}\")\n",
    "        print(\"Confusion matrix:\\n\", confusion_matrix(y_true, preds))\n",
    "        print(classification_report(y_true, preds))\n",
    "        print(f\"Actual positive rate (y): {np.mean(y_true):.6f}\")\n",
    "    else:\n",
    "        print(\"(No labels provided for this split; skipping accuracy/log loss/confusion.)\")\n",
    "    print(f\"Predicted positive rate @ {t:.2f}: {preds.mean():.6f}\")\n",
    "    print(f\"Mean predicted probability: {probs.mean():.6f}\")\n",
    "    print(f\"Min/Max predicted probability: {probs.min():.6f} / {probs.max():.6f}\")\n",
    "    return threshold_table(probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87388794-3fb0-4045-a46f-deadd4f90342",
   "metadata": {},
   "source": [
    "### Refit on Undersampled TRAIN → Validate → TEST Export  \n",
    "Steps performed here:  \n",
    "1. Rebuild the baseline pipeline.  \n",
    "2. Refit on 1:1 undersampled TRAIN (the setup that matched validation).  \n",
    "3. Verify on balanced VALID (target ~0.75 acc / ~0.499 log loss).  \n",
    "4. Predict on TEST and run a sanity check (predicted positive rate ≈ 45–55% if TEST is ~50/50).  \n",
    "5. Export the required `submission_predictions.csv` with exactly two columns: **RowID, Probability**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "02765231-13c2-4d2e-86eb-78f35027b790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Train balance original: {0: 0.959687670237879, 1: 0.04031232976212094}\n",
      "[INFO] Train balance undersampled: {0: 0.5, 1: 0.5}\n",
      "[INFO] Shapes (US): (888, 13) (888,)\n",
      "\n",
      "=== VALID (balanced) check @ t=0.50 ===\n",
      "Accuracy: 0.747748\n",
      "Log loss: 0.498849\n",
      "Confusion matrix:\n",
      " [[86 25]\n",
      " [31 80]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.77      0.75       111\n",
      "           1       0.76      0.72      0.74       111\n",
      "\n",
      "    accuracy                           0.75       222\n",
      "   macro avg       0.75      0.75      0.75       222\n",
      "weighted avg       0.75      0.75      0.75       222\n",
      "\n",
      "Pred pos rate @0.50: 0.472973\n",
      "Mean prob: 0.484102   Min/Max: 0.000000 / 1.000000\n",
      "\n",
      "=== TEST sanity (t=0.50) ===\n",
      "Pred pos rate @0.50: 0.4522\n",
      "Mean prob: 0.4404   Min/Max: 0.0000 / 1.0000\n",
      "[OK] Wrote 230 rows to submission_predictions.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowID</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>0.529412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>0.647059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>0.411765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>107</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>109</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>110</td>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowID  Probability\n",
       "0    101     0.529412\n",
       "1    102     0.000000\n",
       "2    103     0.588235\n",
       "3    104     0.647059\n",
       "4    105     0.411765\n",
       "5    106     0.000000\n",
       "6    107     0.058824\n",
       "7    108     0.000000\n",
       "8    109     0.882353\n",
       "9    110     0.588235"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# refit baseline on 1:1 undersampled train, verify on VALID, export TEST predictions\n",
    "# CONFIG \n",
    "TEST_CSV_PATH = \"NHANES_Data_P1_MI_test.csv\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Build the EXACT baseline pipeline (preprocess -> kNN k=17,p=2,uniform)\n",
    "# Assumes you already defined `preprocess` (your ColumnTransformer with imputer+scaler+OHE)\n",
    "baseline_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"clf\", KNeighborsClassifier(n_neighbors=17, p=2, weights=\"uniform\"))\n",
    "])\n",
    "\n",
    "# Recreate the 1:1 undersampled training set\n",
    "pos_idx = y_train[y_train == 1].index\n",
    "neg_idx = y_train[y_train == 0].index\n",
    "\n",
    "n_pos = len(pos_idx)\n",
    "rng = np.random.RandomState(RANDOM_STATE)\n",
    "neg_keep = rng.choice(neg_idx, size=n_pos, replace=False)\n",
    "keep_idx = np.concatenate([pos_idx, neg_keep])\n",
    "\n",
    "X_train_us = X_train.loc[keep_idx]\n",
    "y_train_us = y_train.loc[keep_idx]\n",
    "\n",
    "perm = rng.permutation(len(X_train_us))\n",
    "X_train_us = X_train_us.iloc[perm]\n",
    "y_train_us = y_train_us.iloc[perm]\n",
    "\n",
    "print(\"[INFO] Train balance original:\", y_train.value_counts(normalize=True).to_dict())\n",
    "print(\"[INFO] Train balance undersampled:\", y_train_us.value_counts(normalize=True).to_dict())\n",
    "print(\"[INFO] Shapes (US):\", X_train_us.shape, y_train_us.shape)\n",
    "\n",
    "# Fit baseline on UNDERSAMPLED train\n",
    "baseline_pipe.fit(X_train_us, y_train_us)\n",
    "\n",
    "# Validate on your balanced validation split\n",
    "probs_val = baseline_pipe.predict_proba(X_valid_bal)[:, 1]\n",
    "preds_val = (probs_val >= 0.5).astype(int)\n",
    "print(\"\\n=== VALID (balanced) check @ t=0.50 ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_valid_bal, preds_val):.6f}\")\n",
    "print(f\"Log loss: {log_loss(y_valid_bal, probs_val):.6f}\")\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_valid_bal, preds_val))\n",
    "print(classification_report(y_valid_bal, preds_val))\n",
    "print(f\"Pred pos rate @0.50: {preds_val.mean():.6f}\")\n",
    "print(f\"Mean prob: {probs_val.mean():.6f}   Min/Max: {probs_val.min():.6f} / {probs_val.max():.6f}\")\n",
    "\n",
    "# Load TEST, keep ID, drop only ID (and label if present)\n",
    "test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "\n",
    "# ID column detection\n",
    "candidate_id_cols = [\"RowID\", \"SEQN\", \"row_id\", \"ID\", \"Id\", \"id\"]\n",
    "id_col = next((c for c in candidate_id_cols if c in test_df.columns), None)\n",
    "RowID = (test_df[id_col] if id_col else test_df.index.to_series()).reset_index(drop=True).rename(\"RowID\")\n",
    "\n",
    "X_test = test_df.drop(columns=[id_col] if id_col else []).copy()\n",
    "\n",
    "# drop label if the test file unexpectedly contains one\n",
    "for label_col in [\"MI\", \"Outcome\", \"Label\", \"target\", \"y\"]:\n",
    "    if label_col in X_test.columns:\n",
    "        X_test = X_test.drop(columns=[label_col])\n",
    "\n",
    "# Predict TEST and export\n",
    "probs_test = baseline_pipe.predict_proba(X_test)[:, 1]\n",
    "preds_test = (probs_test >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n=== TEST sanity (t=0.50) ===\")\n",
    "print(f\"Pred pos rate @0.50: {preds_test.mean():.4f}\")\n",
    "print(f\"Mean prob: {probs_test.mean():.4f}   Min/Max: {probs_test.min():.4f} / {probs_test.max():.4f}\")\n",
    "\n",
    "out_df = pd.DataFrame({\"RowID\": RowID, \"Probability\": probs_test})\n",
    "out_df = out_df[[\"RowID\", \"Probability\"]]\n",
    "out_df.to_csv(\"submission_predictions.csv\", index=False)\n",
    "print(f\"[OK] Wrote {len(out_df):,} rows to submission_predictions.csv\")\n",
    "display(out_df.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
